# DistilIQA: Distilling Vision Transformers for no-reference perceptual CT Image Quality Assessment

No-reference image quality assessment (IQA) is a critical step in medical image analysis, with the objective of predicting perceptual image quality without the necessity for a pristine reference image. The application of no-reference IQA to CT scans is valuable to provide an automated and objective approach to assessing scan quality, optimizing radiation dosage, and enhancing overall healthcare efficiency. In this paper, we introduce DistilIQA, a novel distilled Vision Transformer network designed for no-reference CT image quality assessment. DistilIQA integrates convolutional operations and multi-head self-attention mechanisms by incorporating a powerful convolutional stem at the beginning of the traditional ViT network. Furthermore, we present a two-step  distillation methodology (presented in the Figure below) aimed at improving network performance and efficiency. In the initial step, a "teacher ensemble network" is constructed by training five vision Transformer networks through a five-fold division schema. In the second step, a "student network", comprising of a single Vision Transformer, is trained using the original labeled dataset and the predictions generated by the teacher network as new labels. DistilIQA is evaluated in the task of quality score prediction from low-dose chest CT scans obtained from the LDCT and Projection data of the Cancer Imaging Archive, along with low-dose abdominal CT images from the LDCTIQAC2023 Grand Challenge. Our results demonstrate DistilIQA`s remarkable performance in both bechmarks, surpassing the capabilities of various CNNs and Transformer architectures. Moreover, our comprehensive experimental analysis demonstrates the effectiveness of incorporating convolutional operations within the ViT architecture and highlights the advantages of our distillation methodology.  

![alt text](https://github.com/mariabaldeon/DistilIQA/blob/main/Images/Framework.jpg)

# Requirements
* Python 3.10
* PyTorch 2.0.0 
* Numpy 1.26.0
* Simpleitk 2.3.0
* albumentations 1.3.1
* einops 0.6.1
* scikit-image 0.21.0
* scikit-learn 1.3.1
* torchvision 0.15.2
* scipy 1.11.2

# Dataset
The LDCT and Projection dataset with the chest CT scans is avaiable [here](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=52758026). The LDCTIQAC2023 dataset with the abdominal CT scans is available [here](https://ldctiqac2023.grand-challenge.org/)

# Preprocessing
For the LDCT and Projection dataset you have to first create the images at different doses, and calculate the groundtruth score for each. To perform the preprocessing run: 
```
nohup python3 main.py --task preprocessing & 
```
The code assumes the chest dataset is located in Datasets/LDCTProjection . If it is in another directory, specify the path using the --dataLDCT argument. The images at different doses will be saved at that directory. The code outputs json files with the path and ground truth score to each image. In the preprocessing folder you can find the specific images used for training and testing.
# Training the teacher network 
To carry out the training run: 
```
nohup python3 main.py --task train & 
```
The code assumes the training dataset is located in the directory Datasets/Train. If it is in another directory, specify the path using the --dataTrain argument. The training will be performed in the five folds. For each fold two folders named 2d_training_logs and 3d_training_logs will appear. Inside the folders, the training logs and weights wil be saved for the 2d and 3d CNNs. 

# Training the student network 

# Evaluation
To carry out the evaluation run: 
```
nohup python3 main.py --task evaluate & 
```
The code assumes the testing dataset is located in the directory Datasets/Test. If it is in another directory, specify the path using the --dataTest argument.The code will evaluate the 2D CNN ensemble, 3D CNN ensemble, and PPZSeg-Net. Evaluation metrics will be saved in a .csv file in a folder named Evaluation_metrics. The evaluation  metrics considered are the Dice similarity coefficient (DS) and Haussdorff distance (HD). These metrices will be calculated for the 2D CNN ensemble, 3D CNN ensemble, and PPZSeg-Net. The segmentation results will be saved in the folders Results_2D.mat, Results_3D.mat, and Results_PPZSegNet.mat for the 2D CNN ensemble, 3D CNN ensemble, and PPZSeg-Net, respectively. The trained weights from this work should be located in the directory Networks/weights (link to weights: [link](https://drive.google.com/drive/folders/1wW_aBqUAe9g6eQCN9de1ILyDLg0dGPb0?usp=share_link) ). These weights will  be used for evaluation. If you wan to use other weights, locate them in this folder with the corresponding name k{fold}_{network}D.hdf5, where fold refers to the fold trained on and network to the type of network 2D or 3D.  

# Citation
If you use this code in your research, please cite our paper.
```
@article{wei2multi,
  title={A Multi-object Deep Neural Network Architecture to detect Prostate Anatomy in T2-weighted MRI: Performance Evaluation},
  author={Wei, Zhouping and Baldeon Calisto, Maria and Abudalou, Shatha and Yilmaz, Yasin and Gage, Kenneth and Pow-Sang, Julio M and Balagurunathan, Yoganand},
  journal={Frontiers in Nuclear Medicine},
  volume={2},
  pages={45},
  publisher={Frontiers}
}
```
