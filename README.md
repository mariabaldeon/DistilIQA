# DistilIQA: Distilling Vision Transformers for no-reference perceptual CT Image Quality Assessment

No-reference image quality assessment (IQA) is a critical step in medical image analysis, with the objective of predicting perceptual image quality without the necessity for a pristine reference image. The application of no-reference IQA to CT scans is valuable to provide an automated and objective approach to assessing scan quality, optimizing radiation dosage, and enhancing overall healthcare efficiency. In this paper, we introduce DistilIQA, a novel distilled Vision Transformer network designed for no-reference CT image quality assessment. DistilIQA integrates convolutional operations and multi-head self-attention mechanisms by incorporating a powerful convolutional stem at the beginning of the traditional ViT network. Furthermore, we present a two-step  distillation methodology (presented in the Figure below) aimed at improving network performance and efficiency. In the initial step, a "teacher ensemble network" is constructed by training five vision Transformer networks through a five-fold division schema. In the second step, a "student network", comprising of a single Vision Transformer, is trained using the original labeled dataset and the predictions generated by the teacher network as new labels. DistilIQA is evaluated in the task of quality score prediction from low-dose chest CT scans obtained from the LDCT and Projection data of the Cancer Imaging Archive, along with low-dose abdominal CT images from the LDCTIQAC2023 Grand Challenge. Our results demonstrate DistilIQA`s remarkable performance in both bechmarks, surpassing the capabilities of various CNNs and Transformer architectures. Moreover, our comprehensive experimental analysis demonstrates the effectiveness of incorporating convolutional operations within the ViT architecture and highlights the advantages of our distillation methodology.  

![alt text](https://github.com/mariabaldeon/DistilIQA/blob/main/Images/Framework.jpg)

# Requirements
* Python 3.10
* PyTorch 2.0.0 
* Numpy 1.26.0
* Simpleitk 2.3.0
* albumentations 1.3.1
* einops 0.6.1
* scikit-image 0.21.0
* scikit-learn 1.3.1
* torchvision 0.15.2
* scipy 1.11.2

# Dataset
The LDCT and Projection dataset with the chest CT scans is avaiable [here](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=52758026). The LDCTIQAC2023 dataset with the abdominal CT scans is available [here](https://ldctiqac2023.grand-challenge.org/)

# Preprocessing
For the LDCT and Projection dataset you have to first create the images at different doses, and calculate the groundtruth score for each. To perform the preprocessing run: 
```
nohup python3 main.py --task preprocessing & 
```
The code assumes the chest cases are located in Dataset/LDCT-and-Projection-data. Revise the file Dataset/LDCT-and-Projection-data/dataset_information for details on the cases selected for training and testing. If the cases are in another directory, specify the path using the --data_preprocess_path argument. The images at different doses will be saved at that directory. The code also outputs a json file with the path and ground truth score to each image (dataset.json), json file with the images used for training (train.json), and json file with images used for testing (test.json).

# Training the teacher network 
To carry out the training of the teacher network run: 
```
nohup python3 main.py --task train_teacher & 
```
The training will be performed using a five fold schema. For each fold, a folder named snapshots_fold_{fold} and a file named train_info_fold{fold}.csv will appear. Inside the folder, the weights for the teacher ensemble member will be stored. In the file, information about the loss and overall performance metric for the training and validation set are saved. Use the weights with the highest overall performance in the validation set to form the teacher network.   

# Training the student network 
To carry out the training of the student network run: 
```
nohup python3 main.py --task train_student & 
```
For the code to work, you must locate the weights for the teacher network in the following path Train/weights with the name foldX.hd5, where X refers to the best weight obtained in fold X (eg: for fold 1 use the name fold1.hd5)

# Evaluation
To carry out the evaluation run: 
```
nohup python3 main.py --task train_teacher & 
```
The code assumes the testing dataset is located in the directory Datasets/Test. If it is in another directory, specify the path using the --dataTest argument.The code will evaluate the 2D CNN ensemble, 3D CNN ensemble, and PPZSeg-Net. Evaluation metrics will be saved in a .csv file in a folder named Evaluation_metrics. The evaluation  metrics considered are the Dice similarity coefficient (DS) and Haussdorff distance (HD). These metrices will be calculated for the 2D CNN ensemble, 3D CNN ensemble, and PPZSeg-Net. The segmentation results will be saved in the folders Results_2D.mat, Results_3D.mat, and Results_PPZSegNet.mat for the 2D CNN ensemble, 3D CNN ensemble, and PPZSeg-Net, respectively. The trained weights from this work should be located in the directory Networks/weights (link to weights: [link](https://drive.google.com/drive/folders/1wW_aBqUAe9g6eQCN9de1ILyDLg0dGPb0?usp=share_link) ). These weights will  be used for evaluation. If you wan to use other weights, locate them in this folder with the corresponding name k{fold}_{network}D.hdf5, where fold refers to the fold trained on and network to the type of network 2D or 3D.  

# Citation
If you use this code in your research, please cite our paper.
```
@article{wei2multi,
  title={A Multi-object Deep Neural Network Architecture to detect Prostate Anatomy in T2-weighted MRI: Performance Evaluation},
  author={Wei, Zhouping and Baldeon Calisto, Maria and Abudalou, Shatha and Yilmaz, Yasin and Gage, Kenneth and Pow-Sang, Julio M and Balagurunathan, Yoganand},
  journal={Frontiers in Nuclear Medicine},
  volume={2},
  pages={45},
  publisher={Frontiers}
}
```
