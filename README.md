# DistilIQA: Distilling Vision Transformers for no-reference perceptual CT Image Quality Assessment

No-reference image quality assessment (IQA) is a critical step in medical image analysis, with the objective of predicting perceptual image quality without the necessity for a pristine reference image. The application of no-reference IQA to CT scans is valuable to provide an automated and objective approach to assessing scan quality, optimizing radiation dosage, and enhancing overall healthcare efficiency. In this paper, we introduce DistilIQA, a novel distilled Vision Transformer network designed for no-reference CT image quality assessment. DistilIQA integrates convolutional operations and multi-head self-attention mechanisms by incorporating a powerful convolutional stem at the beginning of the traditional ViT network. Furthermore, we present a two-step  distillation methodology (presented in the Figure below) aimed at improving network performance and efficiency. In the initial step, a "teacher ensemble network" is constructed by training five vision Transformer networks through a five-fold division schema. In the second step, a "student network", comprising of a single Vision Transformer, is trained using the original labeled dataset and the predictions generated by the teacher network as new labels. DistilIQA is evaluated in the task of quality score prediction from low-dose chest CT scans obtained from the LDCT and Projection data of the Cancer Imaging Archive, along with low-dose abdominal CT images from the LDCTIQAC2023 Grand Challenge. Our results demonstrate DistilIQA`s remarkable performance in both bechmarks, surpassing the capabilities of various CNNs and Transformer architectures. Moreover, our comprehensive experimental analysis demonstrates the effectiveness of incorporating convolutional operations within the ViT architecture and highlights the advantages of our distillation methodology.  

![alt text](https://github.com/mariabaldeon/DistilIQA/blob/main/Images/Framework.jpg)

# Requirements
* Python 3.10
* PyTorch 2.0.0 
* Numpy 1.26.0
* Simpleitk 2.3.0
* albumentations 1.3.1
* einops 0.6.1
* scikit-image 0.21.0
* scikit-learn 1.3.1
* torchvision 0.15.2
* scipy 1.11.2

# Dataset
The LDCT and Projection dataset with the chest CT scans is avaiable [here](https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=52758026). The LDCTIQAC2023 dataset with the abdominal CT scans is available [here](https://ldctiqac2023.grand-challenge.org/)

# Preprocessing
For the LDCT and Projection dataset you have to first create the images at different doses, and calculate the groundtruth score for each. To perform the preprocessing run: 
```
nohup python3 main.py --task preprocessing & 
```
The code assumes that the chest cases are located in Dataset/LDCT-and-Projection-data. Revise the file Dataset/LDCT-and-Projection-data/dataset_information for details on the cases selected for training and testing. If the cases are in another directory, specify the path using the --data_preprocess_path argument. The images at different doses will be saved at that directory. The code also outputs a json file with the path and ground truth score to each image (dataset.json), json file with the images used for training (train.json), and json file with images used for testing (test.json).

# Training the teacher network 
To carry out the training of the teacher network run: 
```
nohup python3 main.py --task train_teacher & 
```
The training will be performed using a five fold schema. For each fold, a folder named snapshots_fold_{fold} and a file named train_info_fold_{fold}.csv will appear. Inside the folder, the weights for the teacher ensemble member will be stored. In the file, information about the loss and overall performance metric for the training and validation set are saved. Use the weights with the highest overall performance in the validation set to form the teacher network.   

# Training the student network 
To carry out the training of the student network run: 
```
nohup python3 main.py --task train_student & 
```
For the code to work, you must locate the best weights of the teacher ensemble network in the following path Train/weights with the name fold{fold}.pth for each member (eg: for fold 1 use the name fold1.pth). The code will generate a folder named snapshots_students, and a file named train_info_student.csv. In the snapshots_students folder, the weights obtained from training the student will be stored. In the file, information about the overall performance of the student network, loss, and teacher overall performance are saved. Use the weights with the highest overall performance for evaluation.  

# Evaluation
To carry out the evaluation run: 
```
nohup python3 main.py --task evaluate & 
```
The code assumes the weights for the student network are located in the directory Evaluation/weights. If it is in another directory, specify the path using the --student_weight_path argument.The evaluation metrics will be saved in the  accumulate_metrics.csv file. Furthermore, the prediction and ground truth score for each image in the testing set are saved in the eval_info.csv file. 

# Citation
If you use this code in your research, please cite our paper.
@article{BALDEONCALISTO2024108670,
title = {DistilIQA: Distilling vision transformers for no-reference perceptual CT image quality assessment},
journal = {Computers in Biology and Medicine},
pages = {108670},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108670},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524007558},
author = {Maria Baldeon-Calisto and Francisco Rivera-Velastegui and Susana K. Lai-Yuen and Daniel Riofrío and Noel Pérez-Pérez and Diego Benítez and Ricardo Flores-Moyano}}
